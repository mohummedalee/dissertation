\section{Methods}
We now describe our methodology for measuring the delivery of Facebook ads. 
%
At a high level, our goal is to run groups of ads where we vary a particular feature, with the goal of then measuring how changing that feature skews the set of users the Facebook platform delivers the ad to.
%
To do so, we need to carefully control which users are in our target audience.
%
We also need to develop a methodology to measure the ad delivery skew along racial lines, which, unlike gender, is not provided by Facebook's existing reporting tools.
%
We detail how we achieve that in the following sections.

\subsection{Audience selection}\label{sec:audiences}

When running ads, we often wish to control exactly which ad auctions we are participating in.  
%
For example, if we are running multiple instances of the same ad (\eg to establish statistical confidence), we do not want the instances to be competing against each other.
% 
To this end, we use random PII-based custom audiences, where we randomly select U.S. Facebook users to be included in mutually-exclusive audiences.
%
By doing so, we can ensure that our ads are only competing against each other in the cases where we wish them to.
%
We also replicate some of the experiments while targeting all U.S. users to ensure that the effects do not only exist when custom audiences are targeted. 
%
As we show later in Section~\ref{sec:analysis}, we observe equivalent skews in these scenarios, which leads us to believe that preventing internal competition between our own ads is not crucial to measure the resulting skews.

\parb{Generating custom audiences} 
We create each custom audience by randomly generating 20 lists of 1,000,000 distinct, valid North American phone numbers (\texttt{+1 XXX XXX XXXX}, using known-valid area codes). 
%
Facebook reported that they were able to match approximately 220,000 users on each of the 20 lists we uploaded.

Initially, we used these custom audiences directly to run ads, but while conducting the experiments we noticed that---even though we specifically target only North American phone numbers---many ads were delivered to users outside of North America.
%
This could be caused by users traveling abroad, users registering with fake phone numbers or with online phone number services, or for other reasons, whose investigation is outside the scope of this paper.
%
Therefore, for all the experiments where we target custom audiences, we additionally limit them to people located in the U.S.

\subsection{Data collection}
Once one of our ad campaigns is run, we use the Facebook Marketing API to obtain the delivery performance statistics of the ad every two minutes.
%
When we make this request, we ask Facebook to break down the ad delivery performance according to the attribute of study (age, gender, or location).
%
Facebook's response to each query features the following fields, among others, for each of the demographic attributes that we requested:
\begin{packed_itemize}
%
\item{\texttt{impressions}:} The number of times the ad was shown
%
\item{\texttt{reach}:} The number of unique users the ad was shown to
%
\item{\texttt{clicks}:} The number of clicks the ad has received
%
\item{\texttt{unique\_clicks}:} The number of unique users who clicked 
%
\end{packed_itemize}
%
Throughout the rest of the paper, we use the \texttt{reach} value when examining delivery; thus, when we report ``Fraction of men in the audience'' we calculate this as the \texttt{reach} of men divided by the sum of the \texttt{reach} of men and the \texttt{reach} of women (see Section~\ref{sec:binaries} for discussion on using binary values for gender). 
%

\begin{table*}
\centering
\ra{1.3}
\begin{tabular}{p{3.75cm}cccccccc}\toprule
\multirow{2}{*}{\bf DMA(s)~\cite{NeilsonDMARegions} } & \multicolumn{2}{c}{\bf \# Records ($A$)} & \phantom{ab} &  \multicolumn{2}{c}{\bf \# Records ($B$) } & \phantom{ab} & \multicolumn{2}{c}{\bf \# Records ($C$) }\\
\cmidrule{2-3} \cmidrule{5-6} \cmidrule{8-9}
 & {\bf White} & {\bf Black} && {\bf White} & {\bf Black} && {\bf White} & {\bf Black} \\
\midrule
\multirow{2}{*}{\parbox{3.75cm}{
%\vskip0.05cm
Wilmington,\\
Raleigh--Durham\\
}} & \multirow{2}{*}{400,000} & \multirow{2}{*}{0} && \multirow{2}{*}{0} & \multirow{2}{*}{400,000} && \multirow{2}{*}{900,002} & \multirow{2}{*}{0}\\
& & & & & & & & \\
\multirow{2}{*}{\parbox{3.75cm}{
%\vskip-0.3cm
Greenville-Spartanburg,\\
Greenville-New Bern,\\
Charlotte, Greensboro
\\
}} & \multirow{2}{*}{0} & \multirow{2}{*}{400,000} && \multirow{2}{*}{400,000} & \multirow{2}{*}{0} && \multirow{2}{*}{0} & \multirow{2}{*}{892,097}\\
\phantom{ab} & & & & & & & & \\

\bottomrule
\end{tabular}
%\caption{Overview of the North Carolina custom audiences used to measure racial delivery.  We divide the most populated DMAs in the state into two groups, and in each group, create audiences of both White and Black registered voters with $\sim$450,000 users of the same race.  We then use the statistics Facebook reports about delivery by DMAs to infer delivery by race. \todo{UPDATE -- AM}}
\label{stats:race}
\caption{Overview of the North Carolina custom audiences used to measure racial delivery.  We divide the most populated DMAs in the state into two sets, and create three audiences each with one race per DMA set.  Audiences $A$ and $B$ are disjoint from each other; audience $C$ contains the voters from $A$ with additional white voters from the first DMA set and Black voters from the second DMA set.   We then use the statistics Facebook reports about delivery by DMAs to infer delivery by race.}\label{stats:race}
\end{table*}

\subsection{Measuring racial ad delivery}\label{subsec:race-audiences}
The Facebook Marketing API allows advertisers to request a breakdown of ad delivery performance along a number of axes but it does not provide a breakdown based on race.
%
However, for the purposes of this work, we are able to measure the ad delivery breakdown along racial lines by using location (Designated Market Area, or DMA\footnote{Designated Market Areas~\cite{NeilsonDMARegions} are groups of U.S. counties that Neilson defines as ``market areas''; they were originally used to signify a region where users receive similar broadcast television and radio stations.  Facebook reports ad delivery by location using DMAs, so we use them here as well.}) as a proxy.

% Similar to prior work~\cite{speicher-2018-targeted}, we obtain voter records from North Carolina; these are publicly available records that have the name, address, race, and often phone number of each registered voter in the state.
% %
% We partition the most populated North Carolina DMAs into two sets; for the exact DMAs, please see Table~\ref{stats:race}.
% %, one for each racial group considered in our experiments -- White and Black.
% %
% % To maintain fairly equal audience sizes, we partition the most populated North Carolina DMA regions into two groupings that have roughly the same number of users from each racial group that we consider: White and Black.
% %
% We ensure that each racial group (white and Black) from a set of DMAs has a matching number of records of the other group in the other set of DMAs. %TODO
% %
% We sample three audiences ($A$, $B$, and $C$) that fit these constraints from the voter records and upload as separate Custom Audiences to Facebook.\footnote{Unfortunately, Facebook does not report the number of these users who match as we use multiple PII fields in the upload file~\cite{venkatadri-2018-targeting}.}
% %
% Audiences $A$ and $B$ are disjoint from each other; audience $C$ contains the voters from $A$ with additional white voters from the first DMA set and Black voters from the second DMA set.
% %
% We create audiences in this way to be able to test both ``flipped'' versions of the audiences ($A$ and $B$), as well as large audiences with as many users as possible ($C$); we created audience $B$ as large as possible (exhausting all voters who fit the necessary criteria), and sampled audience $A$ to match its size.
% % pproximately 900,000 users for each race from their corresponding DMAs, we split these audiences into two, in order to support running multiple ads in parallel on non-overlapping audiences.
% %
% % We upload the voter data from each of these four lists as separate Custom Audiences to Facebook.
% %
% The details of the resulting audiences are shown in Table~\ref{stats:race}.

% When we run ads where we want to examine the ad delivery along racial lines, we run the ads to one audience ($A$, $B$, or $C$).
% %
% We then request that Facebook's Marketing API deliver us results broken down by DMA.
% %
% Because we selected DMAs to be a proxy for race, we can use the results to infer which custom audience they were originally in, allowing us to determine the racial makeup of the audience who saw (and clicked on) the ad.
% %
% Note that in experiments that involve measuring racial skew all ads target the same target audience.
% %
% The limited number of registered voters does not allow us to create many large, disjoint custom audiences like we do in other experiments. 
% %
% However, as we show with ads targeting all U.S. users, internal competition does not appear to influence the results.


% \subsubsection{Ad campaigns}
% We use the Facebook Ad API described in Section~\ref{subsec:fb-impl} to create all ads for our experiments and to collect data on their delivery.
% %
% We carefully control for any time-of-day effects that might be present due to different user demographics using Facebook at different times of the day: for any given experiment, we run all ads at the same time to ensure that any such effects are experienced equally by all ads.
% %
% Unless otherwise noted, we used the following settings:
% %
% \begin{packed_itemize}
% %
% \item{\em Objective:} Consideration\textrightarrow Traffic\footnote{This target is defined as: Send more people to a destination on or off Facebook such as a website, app, or Messenger conversation.}
% %
% \item{\em Optimization Goal:} Link Clicks
% %
% \item{\em Traffic destination:} An external website (that depends on the ads run)
% %
% \item{\em Creative:} All of our ads had a single image and text relevant to the ad.
% %2.576
% \item{\em Audience selection:} We use custom audiences for many of our ads, as described in Section~\ref{sec:audiences}, and further restrict them to adult (18+) users of all genders residing in the United States.  For other ads, we target all U.S. users age 18 or older.
% %
% \item{\em Budget:} We ran most ads with a budget of \$20 per day, and stopped them typically after six hours. 
% %
% \end{packed_itemize}

% \subsubsection{Measuring and comparing audiences}
% \label{sec:races}
% We now describe the measurements we make during our experiments and how we compute their confidence intervals.

% \para{Binary values of gender and race}\label{sec:binaries}
% Facebook's marketing API reports ``female'', ``male'', and ``uncategorized'' as the possible values for gender.
% %
% Facebook's users self-report their gender, and the available values are ``female'', ``male'', and ``custom''.
% %
% The latter allows the user to manually type in their gender (with 60 predefined gender identities suggested through auto-complete functionality) and select the preferred pronoun from ``female - her'', ``male - him'', and ``neutral - them''.
% %
% Across our experiments, we observe that up to 1\% of the audiences are reported as ``uncategorized'' gender.
% %
% According to Facebook's documentation this represents the users who did not not list their gender.\footnote{https://www.facebook.com/business/help/151999381652364}
% %
% We do not know whether the ``uncategorized'' gender also features users with self-reported ``custom'' gender.
% %
% Thus, in this work we only consider the self-reported binary gender values of ``female'' and ``male''.

% Further, when considering racial bias, we use the self-reported information from voter records.
% %
% The data we obtained has 7,560,885 individuals, with 93\% reporting their race as either Black or White.
% %
% Among those, less than 1\% report their ethnicity as ``Hispanic/Latino''.
% %
% Thus, in this work, we only target the individuals with self-reported race of White or Black.
% %
% However, when running our experiments measuring race (and targeting specific DMAs), we observe that a fraction ($\sim$10\%) of our ads are delivered to audiences outside of our predefined DMAs, thus making it impossible for us to infer their race.
% %
% This fraction remains fairly consistent across our experiments regardless of what we advertise, thus introducing the same amount of noise across our measurements.
% %
% This is not entirely unexpected, as we are targeting users directly, and those users may be traveling, may have moved, may have outdated information in the voter file, etc.
% %

% We do not claim that gender or race are binary, but choose to focus the analysis on users who self-reported their gender as ``female'' or ``male'' and race as ``Black'' or ``White''.
% %
% This way, we report the observable skew in delivery only along these axes. We recognize that delivery can be \emph{further} skewed with respect to gender of non-binary users and/or users of other races in a way that remains unreported in this work.
% %
% %We leave a more complete analysis to future work and meanwhile encourage the reader to investigate the delivery statistics dataset which we publish along with this article and which reports the delivery to users of ``unknown'' gender and race.
% %

% \para{Measuring statistical significance} 
% Using the binary race and gender features, throughout this work, we describe the audiences by the fraction of male users and the fraction of white users.
% %
% We calculate the lower and upper limits of the 99\% confidence interval around this fraction using the method recommended by Agresti and Coull~\cite{agresti-1998-approximate}, defined in Equation~\ref{eq:single_99}:
% % \begin{equation}
% % CI = 2.576 \cdot \sqrt{\frac{\hat{p}(1-\hat{p})}{n}},
% % \label{eq:single_99}
% % \end{equation}
% \begin{equation}
% \begin{aligned}
% L.L. &= \frac{\hat{p}+\frac{z^2_{\alpha/{2}}}{2n}-z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{{z^2_{\alpha/2}}}{4n^2}}}{1+{z^2_{\alpha/2}}/n},\\
% U.L. &= \frac{\hat{p}+\frac{z^2_{\alpha/{2}}}{2n}+z_{\alpha/2}\sqrt{\frac{\hat{p}(1-\hat{p})}{n}+\frac{{z^2_{\alpha/2}}}{4n^2}}}{1+{z^2_{\alpha/2}}/n},
% \end{aligned}
% \label{eq:single_99}
% \end{equation}
% where $L.L.$ is the lower confidence limit, $U.L.$ is the upper confidence limit, $\hat{p}$ is the observed fraction of the audience with the attribute (here: male), $n$ is the size of the audience reached by the ad. 
% %
% To obtain the 99\% interval we set $z_{\alpha/{2}}=2.576$.
% %
% The advantage of using this calculation instead of the more frequently used normal approximation 
% %
% \begin{equation}
% p\pm z_{\alpha/{2}} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}
% \end{equation}
% %
% is that the resulting intervals fall in the $(0, 1)$ range.
% %
% Whenever the confidence intervals around these fractions for two audiences are non-overlapping, we can make a claim that the gender or racial makeups of two audiences are significantly different~\cite{cumming-2005-inference}.
% %
% However, the converse is not true: overlapping confidence intervals do not necessarily mean that the means are not different (see Figure 4 in~\cite{cumming-2005-inference} for explanation).
% %
% In this work we report all the results of our experiments but for easier interpretation emphasize those where the confidence intervals are non-overlapping.
% %
% We further confirm that the non-overlapping confidence intervals represent statistically significant differences, using the difference of proportion test as shown in  Equation~\ref{eq:zscore}:
% \begin{equation}
% Z = \frac{(\hat{p_1}-\hat{p_2})-0}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1}+\frac{1}{n_2})}}
% \label{eq:zscore}
% \end{equation}
% where $\hat{p_1}$ and $\hat{p_2}$ are the fractions of men (white users) in the two audiences that we compare, $n_1$ and $n_2$ are sizes of these audiences, and $\hat{p}$ is the fraction of men (white users) in the two delivery audiences combined.
% %
% All the results we refer to as statistically significant are significant in this test with a $Z$-score of at least $2.576$.

% Note that in experiments where we run multiple instances of an ad targeting disjoint custom audiences, the values of $\hat{p}$ and $n$ are calculated from the sums of reached audiences.